{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11347934,"sourceType":"datasetVersion","datasetId":7100410}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\nprint(\"hello\") \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T07:50:22.062796Z","iopub.execute_input":"2025-04-19T07:50:22.063490Z","iopub.status.idle":"2025-04-19T07:50:27.099912Z","shell.execute_reply.started":"2025-04-19T07:50:22.063463Z","shell.execute_reply":"2025-04-19T07:50:27.099313Z"}},"outputs":[{"name":"stdout","text":"hello\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## importing libararies","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:30:00.636634Z","iopub.execute_input":"2025-04-19T02:30:00.637135Z","iopub.status.idle":"2025-04-19T02:30:00.641155Z","shell.execute_reply.started":"2025-04-19T02:30:00.637111Z","shell.execute_reply":"2025-04-19T02:30:00.640583Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\nnp.random.seed(42)\n\n# Define improved parameters\nBATCH_SIZE = 16  # Increased from 8\nEPOCHS = 200     # Increased from 100\nLATENT_DIM = 256  # Increased from 128\nNUM_CLASSES = 7\nIMAGE_SIZE = (128, 128)  # Reduced from 256x256 for faster training\nCHANNELS = 3  # RGB images\n\n# Dataset path (update this to your Kaggle dataset path)\nDATA_DIR = \"/kaggle/input/mangoleaf-dataset/dataset\"  # Update this path\n\n# Class names mapping\nCLASS_NAMES = ['ANTHRACNOSE', 'BACTERIAL_CRANKER', 'DIEBACK', \n               'GALL_MILDGE', 'HEALTHY', 'MANGO_SOOTY', 'SOOTY_MOULD']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:30:21.689453Z","iopub.execute_input":"2025-04-19T02:30:21.689737Z","iopub.status.idle":"2025-04-19T02:30:21.694265Z","shell.execute_reply.started":"2025-04-19T02:30:21.689716Z","shell.execute_reply":"2025-04-19T02:30:21.693667Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Define instance normalization layer","metadata":{}},{"cell_type":"code","source":"class InstanceNormalization(layers.Layer):\n    def __init__(self, epsilon=1e-5):\n        super(InstanceNormalization, self).__init__()\n        self.epsilon = epsilon\n\n    def build(self, input_shape):\n        self.scale = self.add_weight(\n            name='scale',\n            shape=input_shape[-1:],\n            initializer='ones',\n            trainable=True)\n        self.offset = self.add_weight(\n            name='offset',\n            shape=input_shape[-1:],\n            initializer='zeros',\n            trainable=True)\n\n    def call(self, x):\n        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n        inv = tf.math.rsqrt(variance + self.epsilon)\n        normalized = (x - mean) * inv\n        return self.scale * normalized + self.offset\n\ndef preprocess_data():\n    \"\"\"Load and preprocess the dataset with improved augmentation.\"\"\"\n    print(\"Loading and preprocessing data...\")\n    \n    # Enhanced data augmentation for more variety\n    datagen = ImageDataGenerator(\n        rescale=1./255,\n        validation_split=0.2,\n        rotation_range=10,      # Small rotations\n        width_shift_range=0.1,  # Small horizontal shifts\n        height_shift_range=0.1, # Small vertical shifts\n        brightness_range=[0.9, 1.1],  # Small brightness variations\n        zoom_range=0.1,         # Small zoom changes\n        horizontal_flip=True    # Horizontal flips\n    )\n    \n    # Load training data\n    train_generator = datagen.flow_from_directory(\n        DATA_DIR,\n        target_size=IMAGE_SIZE,\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        subset='training',\n        shuffle=True\n    )\n    \n    # Load validation data\n    val_generator = datagen.flow_from_directory(\n        DATA_DIR,\n        target_size=IMAGE_SIZE,\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        subset='validation',\n        shuffle=True\n    )\n    \n    return train_generator, val_generator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:30:24.464633Z","iopub.execute_input":"2025-04-19T02:30:24.464897Z","iopub.status.idle":"2025-04-19T02:30:24.472287Z","shell.execute_reply.started":"2025-04-19T02:30:24.464880Z","shell.execute_reply":"2025-04-19T02:30:24.471578Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Generator","metadata":{}},{"cell_type":"code","source":"def build_generator():\n    \"\"\"Build an improved generator model with residual connections and instance normalization.\"\"\"\n    # Input for latent vector\n    latent_input = layers.Input(shape=(LATENT_DIM,))\n    \n    # Input for class label (one-hot encoded)\n    label_input = layers.Input(shape=(NUM_CLASSES,))\n    \n    # Concatenate with wider initial layer\n    x = layers.Concatenate()([latent_input, label_input])\n    x = layers.Dense(16 * 16 * 256)(x)  # Start with larger initial feature map\n    x = InstanceNormalization()(x)  # Use Instance Normalization instead of BatchNorm\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.Reshape((16, 16, 256))(x)\n    \n    # Define residual block with instance normalization\n    def residual_block(x, filters):\n        shortcut = x\n        x = layers.Conv2D(filters, 3, padding='same')(x)\n        x = InstanceNormalization()(x)\n        x = layers.LeakyReLU(0.2)(x)\n        x = layers.Conv2D(filters, 3, padding='same')(x)\n        x = InstanceNormalization()(x)\n        # Add skip connection\n        if shortcut.shape[-1] != filters:\n            shortcut = layers.Conv2D(filters, 1, padding='same')(shortcut)\n        x = layers.Add()([shortcut, x])\n        return x\n    \n    # Upsampling with residual blocks\n    x = layers.Conv2DTranspose(128, 4, strides=2, padding='same')(x)  # 32x32\n    x = InstanceNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = residual_block(x, 128)\n    \n    x = layers.Conv2DTranspose(64, 4, strides=2, padding='same')(x)   # 64x64\n    x = InstanceNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = residual_block(x, 64)\n    \n    x = layers.Conv2DTranspose(32, 4, strides=2, padding='same')(x)   # 128x128\n    x = InstanceNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = residual_block(x, 32)\n    \n    # Output layer with tanh activation\n    output = layers.Conv2D(CHANNELS, 3, padding='same', activation='tanh')(x)\n    \n    model = models.Model([latent_input, label_input], output, name='generator')\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:30:28.184874Z","iopub.execute_input":"2025-04-19T02:30:28.185145Z","iopub.status.idle":"2025-04-19T02:30:28.193385Z","shell.execute_reply.started":"2025-04-19T02:30:28.185125Z","shell.execute_reply":"2025-04-19T02:30:28.192648Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Discriminator","metadata":{}},{"cell_type":"code","source":"def build_discriminator():\n    \"\"\"Build an improved discriminator model with spectral normalization and feature outputs.\"\"\"\n    # Input for image\n    image_input = layers.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], CHANNELS))\n    \n    # Input for class label (one-hot encoded)\n    label_input = layers.Input(shape=(NUM_CLASSES,))\n    \n    # Embed label to match image dimensions\n    label_embedding = layers.Dense(IMAGE_SIZE[0] * IMAGE_SIZE[1])(label_input)\n    label_embedding = layers.Reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 1))(label_embedding)\n    \n    # Concatenate image and label\n    x = layers.Concatenate()([image_input, label_embedding])\n    \n    # Spectral normalization wrapper for Conv2D layers\n    def spectral_norm_conv(x, filters, kernel_size=4, strides=2, padding='same'):\n        # A simplified spectral normalization using layer normalization for stability\n        x = layers.Conv2D(filters, kernel_size, strides=strides, padding=padding)(x)\n        x = layers.LayerNormalization()(x)\n        return x\n    \n    # Apply convolutional layers with spectral normalization\n    x = spectral_norm_conv(x, 32)  # 64x64\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.Dropout(0.25)(x)\n    \n    x = spectral_norm_conv(x, 64)  # 32x32\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.Dropout(0.25)(x)\n    \n    x = spectral_norm_conv(x, 128)  # 16x16\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.Dropout(0.25)(x)\n    \n    x = spectral_norm_conv(x, 256)  # 8x8\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.Dropout(0.25)(x)\n    \n    # Add mini-batch discrimination\n    def minibatch_stddev(x):\n        # A simplified version of minibatch discrimination\n        mean = tf.reduce_mean(x, axis=0, keepdims=True)\n        mean_diff = tf.reduce_mean(tf.abs(x - mean), axis=-1, keepdims=True)\n        mean_diff = tf.tile(mean_diff, [1, 1, 1, 1])  # Match feature dimensions\n        return tf.concat([x, mean_diff], axis=-1)\n    \n    # Add minibatch discrimination layer\n    x = minibatch_stddev(x)\n    \n    # Flatten and feature extraction\n    x = layers.Flatten()(x)\n    features = layers.Dense(512)(x)\n    features = layers.LeakyReLU(0.2)(features)\n    \n    # Output layer\n    output = layers.Dense(1, activation='sigmoid')(features)\n    \n    model = models.Model([image_input, label_input], [output, features], name='discriminator')\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:30:30.893996Z","iopub.execute_input":"2025-04-19T02:30:30.894702Z","iopub.status.idle":"2025-04-19T02:30:30.902525Z","shell.execute_reply.started":"2025-04-19T02:30:30.894677Z","shell.execute_reply":"2025-04-19T02:30:30.901710Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## custom data generator wrapper ","metadata":{}},{"cell_type":"code","source":"class TensorDataGenerator:\n    def __init__(self, keras_generator):\n        self.keras_generator = keras_generator\n        self.n = len(keras_generator)\n        self.batch_size = keras_generator.batch_size\n        self.samples = keras_generator.samples\n        self.class_indices = keras_generator.class_indices\n    \n    def __len__(self):\n        return len(self.keras_generator)\n    \n    def __getitem__(self, idx):\n        images, labels = self.keras_generator[idx]\n        # Convert to TensorFlow tensors\n        return tf.convert_to_tensor(images, dtype=tf.float32), tf.convert_to_tensor(labels, dtype=tf.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:30:34.248408Z","iopub.execute_input":"2025-04-19T02:30:34.249106Z","iopub.status.idle":"2025-04-19T02:30:34.253649Z","shell.execute_reply.started":"2025-04-19T02:30:34.249082Z","shell.execute_reply":"2025-04-19T02:30:34.252853Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Gradient penalty function for WGAN-GP stability","metadata":{"execution":{"iopub.status.busy":"2025-04-19T02:27:54.705612Z","iopub.execute_input":"2025-04-19T02:27:54.706127Z","iopub.status.idle":"2025-04-19T02:27:54.709933Z","shell.execute_reply.started":"2025-04-19T02:27:54.706100Z","shell.execute_reply":"2025-04-19T02:27:54.709040Z"}}},{"cell_type":"code","source":"def gradient_penalty(discriminator, real_images, fake_images, labels):\n    batch_size = tf.shape(real_images)[0]\n    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n    interpolated = real_images * alpha + fake_images * (1 - alpha)\n    \n    with tf.GradientTape() as gp_tape:\n        gp_tape.watch(interpolated)\n        pred, _ = discriminator([interpolated, labels], training=True)\n    \n    grads = gp_tape.gradient(pred, interpolated)\n    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n    gp = tf.reduce_mean((norm - 1.0) ** 2)\n    return gp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:30:39.199369Z","iopub.execute_input":"2025-04-19T02:30:39.199712Z","iopub.status.idle":"2025-04-19T02:30:39.205615Z","shell.execute_reply.started":"2025-04-19T02:30:39.199681Z","shell.execute_reply":"2025-04-19T02:30:39.204890Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## define custom trainig loop","metadata":{}},{"cell_type":"code","source":"def train_gan(generator, discriminator, train_generator, val_generator, epochs, output_dir):\n    \"\"\"Custom training loop for the GAN with stability improvements.\"\"\"\n    # Convert to tensor generators\n    train_gen = TensorDataGenerator(train_generator)\n    val_gen = TensorDataGenerator(val_generator)\n    \n    # Create optimizers with different learning rates for generator and discriminator\n    generator_lr = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=0.00005,  # Reduced learning rate for generator\n        decay_steps=1000,\n        decay_rate=0.95)\n    \n    discriminator_lr = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=0.0002,  # Higher learning rate for discriminator\n        decay_steps=1000,\n        decay_rate=0.95)\n    \n    generator_optimizer = optimizers.Adam(learning_rate=generator_lr, beta_1=0.5, beta_2=0.999)\n    discriminator_optimizer = optimizers.Adam(learning_rate=discriminator_lr, beta_1=0.5, beta_2=0.999)\n    \n    # Loss functions\n    bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n    \n    # Prepare for early stopping with higher patience\n    best_g_loss = float('inf')\n    patience = 25\n    patience_counter = 0\n    \n    # Training metrics history\n    history = {'d_loss': [], 'g_loss': [], 'val_d_loss': [], 'val_g_loss': []}\n    \n    # Fixed noise for generating sample images\n    sample_per_class = 2\n    fixed_noise = tf.random.normal(shape=(NUM_CLASSES * sample_per_class, LATENT_DIM))\n    fixed_labels = np.zeros((NUM_CLASSES * sample_per_class, NUM_CLASSES))\n    for i in range(NUM_CLASSES):\n        for j in range(sample_per_class):\n            fixed_labels[i * sample_per_class + j, i] = 1\n    fixed_labels = tf.convert_to_tensor(fixed_labels, dtype=tf.float32)\n    \n    # Create output directory for samples\n    samples_dir = os.path.join(output_dir, \"samples\")\n    os.makedirs(samples_dir, exist_ok=True)\n    \n    # Lambda values for different loss components\n    lambda_gp = 10.0        # Gradient penalty weight\n    lambda_feat = 10.0      # Feature matching weight\n    lambda_div = 0.1        # Diversity weight\n    \n    # Training loop\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        start_time = time.time()\n        \n        # Initialize metrics\n        train_d_losses = []\n        train_g_losses = []\n        train_d_accs = []\n        \n        # Calculate noise standard deviation (decreases over time for instance noise)\n        noise_std = max(0.0, 0.05 * (1.0 - epoch / (EPOCHS * 0.3)))\n        \n        # Training\n        for batch_idx in range(len(train_gen)):\n            # Get a batch of real images\n            real_images, one_hot_labels = train_gen[batch_idx]\n            batch_size = real_images.shape[0]\n            \n            # Skip batches with unexpected sizes (last batch might be smaller)\n            if batch_size != BATCH_SIZE:\n                continue\n            \n            # Generate random noise\n            random_latent_vectors = tf.random.normal(shape=(batch_size, LATENT_DIM))\n            \n            # Generate fake images\n            generated_images = generator([random_latent_vectors, one_hot_labels], training=True)\n            \n            # Add instance noise to both real and fake images (decreases over time)\n            real_images_noisy = real_images + tf.random.normal(tf.shape(real_images), mean=0.0, stddev=noise_std)\n            generated_images_noisy = generated_images + tf.random.normal(tf.shape(generated_images), mean=0.0, stddev=noise_std)\n            \n            # Use label smoothing for real labels (0.9 instead of 1.0)\n            real_labels = tf.ones((batch_size, 1)) * 0.9\n            fake_labels = tf.zeros((batch_size, 1))\n            \n            # Train discriminator\n            with tf.GradientTape() as d_tape:\n                # Discriminator on real images\n                real_output, real_features = discriminator([real_images_noisy, one_hot_labels], training=True)\n                # Discriminator on fake images\n                fake_output, fake_features = discriminator([generated_images_noisy, one_hot_labels], training=True)\n                \n                # Calculate discriminator loss\n                d_loss_real = bce_loss(real_labels, real_output)\n                d_loss_fake = bce_loss(fake_labels, fake_output)\n                d_loss = d_loss_real + d_loss_fake\n                \n                # Add gradient penalty\n                gp = gradient_penalty(discriminator, real_images, generated_images, one_hot_labels)\n                d_loss += lambda_gp * gp\n            \n            # Calculate discriminator accuracy\n            d_acc = (tf.reduce_mean(tf.cast(real_output > 0.5, tf.float32)) * 0.5 + \n                     tf.reduce_mean(tf.cast(fake_output < 0.5, tf.float32)) * 0.5)\n            train_d_accs.append(d_acc.numpy())\n            \n            # Calculate gradients and update discriminator weights\n            d_gradients = d_tape.gradient(d_loss, discriminator.trainable_variables)\n            # Clip gradients to prevent exploding gradients\n            d_gradients, _ = tf.clip_by_global_norm(d_gradients, 1.0)\n            discriminator_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n            \n            # Store discriminator loss\n            train_d_losses.append(d_loss.numpy())\n            \n            # Train generator every step (removing d_acc < 0.8 condition)\n            # Generate two different noise vectors for diversity loss\n            noise1 = tf.random.normal(shape=(batch_size, LATENT_DIM))\n            noise2 = tf.random.normal(shape=(batch_size, LATENT_DIM))\n            \n            with tf.GradientTape() as g_tape:\n                # Generate fake images from both noise vectors\n                generated_images1 = generator([noise1, one_hot_labels], training=True)\n                generated_images2 = generator([noise2, one_hot_labels], training=True)\n                \n                # Discriminator output for fake images\n                fake_output1, fake_features1 = discriminator([generated_images1, one_hot_labels], training=True)\n                _, fake_features2 = discriminator([generated_images2, one_hot_labels], training=True)\n                \n                # Basic generator loss (we want discriminator to classify fake images as real)\n                g_loss = bce_loss(real_labels, fake_output1)\n                \n                # Add feature matching loss\n                feature_matching_loss = tf.reduce_mean(tf.abs(fake_features1 - real_features))\n                g_loss += lambda_feat * feature_matching_loss\n                \n                # Add diversity loss (mode seeking)\n                img_distance = tf.reduce_mean(tf.abs(generated_images1 - generated_images2))\n                noise_distance = tf.reduce_mean(tf.abs(noise1 - noise2))\n                diversity_loss = 1.0 / (img_distance / noise_distance + 1e-6)\n                g_loss += lambda_div * diversity_loss\n                \n                # Add L2 regularization to prevent extreme weights\n                l2_reg = tf.add_n([tf.nn.l2_loss(v) for v in generator.trainable_variables])\n                g_loss += 0.0001 * l2_reg\n            \n            # Calculate gradients and update generator weights\n            g_gradients = g_tape.gradient(g_loss, generator.trainable_variables)\n            # Clip gradients to prevent exploding gradients\n            g_gradients, _ = tf.clip_by_global_norm(g_gradients, 1.0)\n            generator_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n            \n            # Store generator loss\n            train_g_losses.append(g_loss.numpy())\n            \n            # Break after one epoch\n            if batch_idx >= len(train_gen) - 1:\n                break\n        \n        # Validation\n        val_d_losses = []\n        val_g_losses = []\n        \n        for batch_idx in range(len(val_gen)):\n            # Get a batch of real images\n            real_images, one_hot_labels = val_gen[batch_idx]\n            batch_size = real_images.shape[0]\n            \n            # Skip batches with unexpected sizes\n            if batch_size != BATCH_SIZE:\n                continue\n            \n            # Generate random noise\n            random_latent_vectors = tf.random.normal(shape=(batch_size, LATENT_DIM))\n            \n            # Generate fake images\n            generated_images = generator([random_latent_vectors, one_hot_labels], training=False)\n            \n            # Evaluate discriminator\n            real_output, real_features = discriminator([real_images, one_hot_labels], training=False)\n            fake_output, fake_features = discriminator([generated_images, one_hot_labels], training=False)\n            \n            # Calculate discriminator loss\n            real_labels = tf.ones((batch_size, 1)) * 0.9\n            fake_labels = tf.zeros((batch_size, 1))\n            d_loss_real = bce_loss(real_labels, real_output)\n            d_loss_fake = bce_loss(fake_labels, fake_output)\n            d_loss = d_loss_real + d_loss_fake\n            \n            # Evaluate generator with feature matching\n            g_loss = bce_loss(real_labels, fake_output)\n            feature_matching_loss = tf.reduce_mean(tf.abs(fake_features - real_features))\n            g_loss += lambda_feat * feature_matching_loss\n            \n            # Store losses\n            val_d_losses.append(d_loss.numpy())\n            val_g_losses.append(g_loss.numpy())\n            \n            # Break after one validation epoch\n            if batch_idx >= len(val_gen) - 1:\n                break\n        \n        # Calculate average losses\n        avg_train_d_loss = np.mean(train_d_losses)\n        avg_train_g_loss = np.mean(train_g_losses) if train_g_losses else 0\n        avg_train_d_acc = np.mean(train_d_accs)\n        avg_val_d_loss = np.mean(val_d_losses)\n        avg_val_g_loss = np.mean(val_g_losses)\n        \n        # Update history\n        history['d_loss'].append(avg_train_d_loss)\n        history['g_loss'].append(avg_train_g_loss)\n        history['val_d_loss'].append(avg_val_d_loss)\n        history['val_g_loss'].append(avg_val_g_loss)\n        \n        # Print progress\n        time_taken = time.time() - start_time\n        print(f\"d_loss: {avg_train_d_loss:.4f}, g_loss: {avg_train_g_loss:.4f}, d_acc: {avg_train_d_acc:.4f}\")\n        print(f\"val_d_loss: {avg_val_d_loss:.4f}, val_g_loss: {avg_val_g_loss:.4f}, time: {time_taken:.2f}s\")\n        \n        # Check for early stopping\n        if avg_val_g_loss < best_g_loss:\n            best_g_loss = avg_val_g_loss\n            patience_counter = 0\n            # Save best models\n            generator.save(os.path.join(output_dir, \"best_generator_model.keras\"))\n            discriminator.save(os.path.join(output_dir, \"best_discriminator_model.keras\"))\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping triggered at epoch {epoch+1}\")\n                break\n        \n        # Generate and save sample images every 5 epochs\n        if epoch % 5 == 0 or epoch == epochs - 1:\n            generated_images = generator([fixed_noise, fixed_labels], training=False)\n            generated_images = (generated_images * 0.5 + 0.5) * 255  # Rescale to [0, 255]\n            generated_images = generated_images.numpy().astype(np.uint8)\n            \n            # Create a figure to display images\n            plt.figure(figsize=(15, 5))\n            for i in range(NUM_CLASSES * sample_per_class):\n                plt.subplot(sample_per_class, NUM_CLASSES, i + 1)\n                plt.imshow(generated_images[i])\n                class_idx = i // sample_per_class\n                plt.title(CLASS_NAMES[class_idx])\n                plt.axis('off')\n            \n            # Save figure\n            plt.tight_layout()\n            plt.savefig(f\"{samples_dir}/generated_epoch_{epoch+1}.png\")\n            plt.close()\n    \n    # Load best models if early stopping occurred\n    if patience_counter >= patience:\n        generator = tf.keras.models.load_model(os.path.join(output_dir, \"best_generator_model.keras\"))\n        discriminator = tf.keras.models.load_model(os.path.join(output_dir, \"best_discriminator_model.keras\"))\n    \n    return generator, discriminator, history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:30:41.702332Z","iopub.execute_input":"2025-04-19T02:30:41.702606Z","iopub.status.idle":"2025-04-19T02:30:41.725417Z","shell.execute_reply.started":"2025-04-19T02:30:41.702588Z","shell.execute_reply":"2025-04-19T02:30:41.724753Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## generate images","metadata":{}},{"cell_type":"code","source":"def generate_images(generator, num_samples_per_class=10, output_dir=\"generated_images\"):\n    \"\"\"Generate synthetic images for each class and save them.\"\"\"\n    print(f\"Generating {num_samples_per_class} images per class...\")\n    \n    # Create output directories\n    for class_name in CLASS_NAMES:\n        os.makedirs(os.path.join(output_dir, class_name), exist_ok=True)\n    \n    for i, class_name in enumerate(CLASS_NAMES):\n        # Create one-hot encoded labels for this class\n        labels = np.zeros((num_samples_per_class, NUM_CLASSES))\n        labels[:, i] = 1\n        # Convert to tensor\n        labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n        \n        # Generate random noise\n        noise = tf.random.normal([num_samples_per_class, LATENT_DIM])\n        \n        # Generate images\n        generated_images = generator([noise, labels], training=False)\n        \n        # Rescale images from [-1, 1] to [0, 255]\n        generated_images = (generated_images * 0.5 + 0.5) * 255\n        generated_images = generated_images.numpy().astype(np.uint8)\n        \n        # Save images\n        for j in range(num_samples_per_class):\n            output_path = os.path.join(output_dir, class_name, f\"synthetic_{j}.png\")\n            plt.imsave(output_path, generated_images[j])\n            \n    print(f\"Images saved to {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:30:59.315597Z","iopub.execute_input":"2025-04-19T02:30:59.316365Z","iopub.status.idle":"2025-04-19T02:30:59.321804Z","shell.execute_reply.started":"2025-04-19T02:30:59.316340Z","shell.execute_reply":"2025-04-19T02:30:59.321145Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## writing a main function","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"Main function to execute the entire training pipeline.\"\"\"\n    print(\"TensorFlow version:\", tf.__version__)\n    print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n    \n    # Create output directory\n    OUTPUT_DIR = \"/kaggle/working/mango_gan_output\"\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    # Load data\n    train_generator, val_generator = preprocess_data()\n    \n    # Print dataset info\n    print(f\"Found {train_generator.samples} training images\")\n    print(f\"Found {val_generator.samples} validation images\")\n    print(f\"Class names: {train_generator.class_indices}\")\n    \n    # Build models\n    generator = build_generator()\n    discriminator = build_discriminator()\n    \n    # Print model summaries\n    generator.summary()\n    discriminator.summary()\n    \n    # Train the model using custom training loop\n    start_time = time.time()\n    generator, discriminator, history = train_gan(\n        generator, \n        discriminator, \n        train_generator, \n        val_generator, \n        EPOCHS, \n        OUTPUT_DIR\n    )\n    training_time = time.time() - start_time\n    print(f\"Training completed in {training_time/60:.2f} minutes\")\n    \n    # Plot training history\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history['d_loss'], label='Train Discriminator Loss')\n    plt.plot(history['val_d_loss'], label='Val Discriminator Loss')\n    plt.title('Discriminator Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history['g_loss'], label='Train Generator Loss')\n    plt.plot(history['val_g_loss'], label='Val Generator Loss')\n    plt.title('Generator Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, \"training_history.png\"))\n    \n    # Save final models\n    generator.save(os.path.join(OUTPUT_DIR, \"final_generator_model.keras\"))\n    discriminator.save(os.path.join(OUTPUT_DIR, \"final_discriminator_model.keras\"))\n    \n    # Generate a set of synthetic images\n    GENERATED_DIR = os.path.join(OUTPUT_DIR, \"generated_dataset\")\n    generate_images(generator, num_samples_per_class=20, output_dir=GENERATED_DIR)\n    \n    print(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:31:02.432796Z","iopub.execute_input":"2025-04-19T02:31:02.433482Z","iopub.status.idle":"2025-04-19T02:31:02.441090Z","shell.execute_reply.started":"2025-04-19T02:31:02.433460Z","shell.execute_reply":"2025-04-19T02:31:02.440253Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Execute main function\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:31:06.221798Z","iopub.execute_input":"2025-04-19T02:31:06.222066Z","iopub.status.idle":"2025-04-19T02:31:09.043092Z","shell.execute_reply.started":"2025-04-19T02:31:06.222047Z","shell.execute_reply":"2025-04-19T02:31:09.042059Z"}},"outputs":[{"name":"stdout","text":"TensorFlow version: 2.18.0\nGPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nLoading and preprocessing data...\nFound 1438 images belonging to 7 classes.\nFound 357 images belonging to 7 classes.\nFound 1438 training images\nFound 357 validation images\nClass names: {'ANTHRACNOSE': 0, 'BACTERIAL CRANKER': 1, 'DIEBACK': 2, 'GALL MILDGE': 3, 'HEALTHY': 4, 'MANGO SOOTY': 5, 'SOOTY MOULD': 6}\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745029867.655413      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2203761024.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Execute main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_31/1297111316.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Build models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2637687437.py\u001b[0m in \u001b[0;36mbuild_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlatent_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Start with larger initial feature map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInstanceNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use Instance Normalization instead of BatchNorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2993713592.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0minv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling InstanceNormalization.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'instance_normalization' (of type InstanceNormalization). Either the `InstanceNormalization.call()` method is incorrect, or you need to implement the `InstanceNormalization.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nInvalid reduction dimension 2 for input with 2 dimensions. for '{{node moments/mean}} = Mean[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=true](Placeholder, moments/mean/reduction_indices)' with input shapes: [?,65536], [2] and with computed input tensors: input[1] = <1 2>.\u001b[0m\n\nArguments received by InstanceNormalization.call():\n  • args=('<KerasTensor shape=(None, 65536), dtype=float32, sparse=False, name=keras_tensor_3>',)\n  • kwargs=<class 'inspect._empty'>"],"ename":"ValueError","evalue":"Exception encountered when calling InstanceNormalization.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'instance_normalization' (of type InstanceNormalization). Either the `InstanceNormalization.call()` method is incorrect, or you need to implement the `InstanceNormalization.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nInvalid reduction dimension 2 for input with 2 dimensions. for '{{node moments/mean}} = Mean[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=true](Placeholder, moments/mean/reduction_indices)' with input shapes: [?,65536], [2] and with computed input tensors: input[1] = <1 2>.\u001b[0m\n\nArguments received by InstanceNormalization.call():\n  • args=('<KerasTensor shape=(None, 65536), dtype=float32, sparse=False, name=keras_tensor_3>',)\n  • kwargs=<class 'inspect._empty'>","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nimport numpy as np\nimport uuid\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyperparameters\nnum_classes = 7\nlatent_dim = 128  # Increased from 100 to 128\nimage_size = 256  # Resized for computational feasibility\nchannels = 3\nngf = 64  # Generator feature maps\nndf = 64  # Discriminator feature maps\nnum_epochs = 100\nbatch_size = 32\nlr = 0.0002\nbeta1 = 0.5\npatience = 15  # Early stopping patience\nmin_delta = 0.001  # Minimum improvement for early stopping\n\n# Class labels\nclass_names = ['ANTHRACNOSE', 'BACTERIAL_CRANKER', 'DIEBACK', 'GALL_MILDGE', 'HEALTHY', 'MANGO_SOOTY', 'SOOTY_MOULD']\n\n# Data transforms\ntransform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n])\n\n# Load dataset\ndataset = ImageFolder(root='/kaggle/input/mangoleaf-dataset/dataset', transform=transform)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n\n# Generator\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.label_emb = nn.Embedding(num_classes, num_classes)\n        \n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(latent_dim + num_classes, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, ngf // 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf // 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf // 2, ngf // 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf // 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf // 4, channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, z, labels):\n        label_emb = self.label_emb(labels).view(labels.size(0), num_classes, 1, 1)\n        z = z.view(z.size(0), latent_dim, 1, 1)\n        input = torch.cat([z, label_emb], dim=1)\n        return self.model(input)\n\n# Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.label_emb = nn.Embedding(num_classes, num_classes)\n        \n        self.model = nn.Sequential(\n            nn.Conv2d(channels + num_classes, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 16),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 16, 1, 8, 1, 0, bias=False),  # Outputs 1x1\n            nn.Sigmoid()\n        )\n\n    def forward(self, img, labels):\n        label_emb = self.label_emb(labels).view(labels.size(0), num_classes, 1, 1)\n        label_emb = label_emb.repeat(1, 1, img.size(2), img.size(3))\n        input = torch.cat([img, label_emb], dim=1)\n        output = self.model(input)\n        return output.view(-1)  # Flatten to [batch_size]\n\n# Initialize models\ngenerator = Generator().to(device)\ndiscriminator = Discriminator().to(device)\n\n# Load pre-trained weights (if available)\n# Example: generator.load_state_dict(torch.load('pretrained_generator.pth'))\n\n# Loss and optimizers\nadversarial_loss = nn.BCELoss()\noptimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n\n# Early stopping variables\nbest_g_loss = float('inf')\nepochs_no_improve = 0\nearly_stop = False\n\n# Training loop\nfor epoch in range(num_epochs):\n    if early_stop:\n        print(f\"Early stopping triggered at epoch {epoch}\")\n        break\n    \n    epoch_g_loss = 0.0\n    epoch_d_loss = 0.0\n    batches = 0\n    \n    for i, (imgs, labels) in enumerate(dataloader):\n        batch_size = imgs.size(0)\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n        \n        # Ground truth\n        real_label = torch.ones(batch_size).to(device)\n        fake_label = torch.zeros(batch_size).to(device)\n        \n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n        optimizer_D.zero_grad()\n        \n        # Real images\n        real_validity = discriminator(imgs, labels)\n        d_real_loss = adversarial_loss(real_validity, real_label)\n        \n        # Fake images\n        z = torch.randn(batch_size, latent_dim).to(device)\n        gen_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n        gen_imgs = generator(z, gen_labels)\n        fake_validity = discriminator(gen_imgs.detach(), gen_labels)\n        d_fake_loss = adversarial_loss(fake_validity, fake_label)\n        \n        # Total discriminator loss\n        d_loss = (d_real_loss + d_fake_loss) / 2\n        d_loss.backward()\n        optimizer_D.step()\n        \n        # -----------------\n        #  Train Generator\n        # -----------------\n        optimizer_G.zero_grad()\n        \n        # Generate images\n        fake_validity = discriminator(gen_imgs, gen_labels)\n        g_loss = adversarial_loss(fake_validity, real_label)\n        \n        g_loss.backward()\n        optimizer_G.step()\n        \n        # Accumulate losses for epoch\n        epoch_g_loss += g_loss.item()\n        epoch_d_loss += d_loss.item()\n        batches += 1\n        \n        # Print progress\n        if i % 10 == 0:\n            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] \"\n                  f\"D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n    \n    # Average losses for the epoch\n    epoch_g_loss /= batches\n    epoch_d_loss /= batches\n    \n    # Early stopping check\n    if epoch_g_loss < best_g_loss - min_delta:\n        print(f\"New best generator loss: {epoch_g_loss:.4f}. Saving models...\")\n        best_g_loss = epoch_g_loss\n        epochs_no_improve = 0\n        # Save best model weights\n        torch.save(generator.state_dict(), 'best_generator_mangoleaf.pth')\n        torch.save(discriminator.state_dict(), 'best_discriminator_mangoleaf.pth')\n    else:\n        epochs_no_improve += 1\n        print(f\"No improvement in generator loss. Epochs without improvement: {epochs_no_improve}/{patience}\")\n    \n    if epochs_no_improve >= patience:\n        early_stop = True\n    \n    # Save generated images for each class\n    if epoch % 5 == 0:\n        with torch.no_grad():\n            for class_idx, class_name in enumerate(class_names):\n                z = torch.randn(1, latent_dim).to(device)\n                gen_label = torch.tensor([class_idx]).to(device)\n                gen_img = generator(z, gen_label)\n                torchvision.utils.save_image(gen_img, f'generated_{class_name}_epoch_{epoch}.png', normalize=True)\n\n# Save final models\ntorch.save(generator.state_dict(), 'final_generator_mangoleaf.pth')\ntorch.save(discriminator.state_dict(), 'final_discriminator_mangoleaf.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T07:58:33.462177Z","iopub.execute_input":"2025-04-19T07:58:33.462895Z","iopub.status.idle":"2025-04-19T08:30:30.867537Z","shell.execute_reply.started":"2025-04-19T07:58:33.462871Z","shell.execute_reply":"2025-04-19T08:30:30.866573Z"}},"outputs":[{"name":"stdout","text":"[Epoch 0/100] [Batch 0/57] D_loss: 0.7255, G_loss: 4.8431\n[Epoch 0/100] [Batch 10/57] D_loss: 0.1601, G_loss: 5.3553\n[Epoch 0/100] [Batch 20/57] D_loss: 0.1215, G_loss: 7.0801\n[Epoch 0/100] [Batch 30/57] D_loss: 0.2218, G_loss: 5.2242\n[Epoch 0/100] [Batch 40/57] D_loss: 0.0229, G_loss: 5.2852\n[Epoch 0/100] [Batch 50/57] D_loss: 0.1213, G_loss: 3.6710\nNew best generator loss: 5.1927. Saving models...\n[Epoch 1/100] [Batch 0/57] D_loss: 3.9512, G_loss: 4.9021\n[Epoch 1/100] [Batch 10/57] D_loss: 0.1717, G_loss: 3.6316\n[Epoch 1/100] [Batch 20/57] D_loss: 0.0120, G_loss: 5.7335\n[Epoch 1/100] [Batch 30/57] D_loss: 0.6060, G_loss: 2.4729\n[Epoch 1/100] [Batch 40/57] D_loss: 0.0258, G_loss: 6.0825\n[Epoch 1/100] [Batch 50/57] D_loss: 0.3320, G_loss: 6.3974\nNew best generator loss: 4.0937. Saving models...\n[Epoch 2/100] [Batch 0/57] D_loss: 0.2047, G_loss: 5.7458\n[Epoch 2/100] [Batch 10/57] D_loss: 0.1932, G_loss: 3.9486\n[Epoch 2/100] [Batch 20/57] D_loss: 0.0290, G_loss: 3.9508\n[Epoch 2/100] [Batch 30/57] D_loss: 0.4858, G_loss: 1.4595\n[Epoch 2/100] [Batch 40/57] D_loss: 0.0356, G_loss: 10.4640\n[Epoch 2/100] [Batch 50/57] D_loss: 0.0079, G_loss: 7.6847\nNo improvement in generator loss. Epochs without improvement: 1/15\n[Epoch 3/100] [Batch 0/57] D_loss: 1.3833, G_loss: 5.2410\n[Epoch 3/100] [Batch 10/57] D_loss: 0.1539, G_loss: 6.1455\n[Epoch 3/100] [Batch 20/57] D_loss: 0.0475, G_loss: 5.1245\n[Epoch 3/100] [Batch 30/57] D_loss: 0.7098, G_loss: 2.7586\n[Epoch 3/100] [Batch 40/57] D_loss: 0.1155, G_loss: 3.5629\n[Epoch 3/100] [Batch 50/57] D_loss: 0.0376, G_loss: 2.7084\nNo improvement in generator loss. Epochs without improvement: 2/15\n[Epoch 4/100] [Batch 0/57] D_loss: 1.2468, G_loss: 10.7103\n[Epoch 4/100] [Batch 10/57] D_loss: 2.3143, G_loss: 3.1216\n[Epoch 4/100] [Batch 20/57] D_loss: 0.0081, G_loss: 3.2284\n[Epoch 4/100] [Batch 30/57] D_loss: 0.0648, G_loss: 8.5900\n[Epoch 4/100] [Batch 40/57] D_loss: 0.0030, G_loss: 7.8616\n[Epoch 4/100] [Batch 50/57] D_loss: 0.5336, G_loss: 2.5766\nNo improvement in generator loss. Epochs without improvement: 3/15\n[Epoch 5/100] [Batch 0/57] D_loss: 1.4937, G_loss: 2.5937\n[Epoch 5/100] [Batch 10/57] D_loss: 0.6275, G_loss: 1.8814\n[Epoch 5/100] [Batch 20/57] D_loss: 0.2581, G_loss: 3.4275\n[Epoch 5/100] [Batch 30/57] D_loss: 0.0009, G_loss: 7.8874\n[Epoch 5/100] [Batch 40/57] D_loss: 0.0333, G_loss: 3.0396\n[Epoch 5/100] [Batch 50/57] D_loss: 0.0162, G_loss: 7.2858\nNo improvement in generator loss. Epochs without improvement: 4/15\n[Epoch 6/100] [Batch 0/57] D_loss: 0.4137, G_loss: 0.9663\n[Epoch 6/100] [Batch 10/57] D_loss: 1.5258, G_loss: 2.9757\n[Epoch 6/100] [Batch 20/57] D_loss: 0.7184, G_loss: 0.1549\n[Epoch 6/100] [Batch 30/57] D_loss: 0.0015, G_loss: 12.2449\n[Epoch 6/100] [Batch 40/57] D_loss: 0.0068, G_loss: 4.5940\n[Epoch 6/100] [Batch 50/57] D_loss: 0.0091, G_loss: 7.3609\nNo improvement in generator loss. Epochs without improvement: 5/15\n[Epoch 7/100] [Batch 0/57] D_loss: 0.7436, G_loss: 10.4716\n[Epoch 7/100] [Batch 10/57] D_loss: 0.4239, G_loss: 9.0920\n[Epoch 7/100] [Batch 20/57] D_loss: 0.0003, G_loss: 8.7706\n[Epoch 7/100] [Batch 30/57] D_loss: 0.0493, G_loss: 7.9152\n[Epoch 7/100] [Batch 40/57] D_loss: 1.1713, G_loss: 5.2593\n[Epoch 7/100] [Batch 50/57] D_loss: 0.0055, G_loss: 5.4887\nNo improvement in generator loss. Epochs without improvement: 6/15\n[Epoch 8/100] [Batch 0/57] D_loss: 0.5177, G_loss: 1.0520\n[Epoch 8/100] [Batch 10/57] D_loss: 0.0011, G_loss: 6.8769\n[Epoch 8/100] [Batch 20/57] D_loss: 0.5407, G_loss: 3.1955\n[Epoch 8/100] [Batch 30/57] D_loss: 0.0039, G_loss: 13.4002\n[Epoch 8/100] [Batch 40/57] D_loss: 0.0921, G_loss: 3.1910\n[Epoch 8/100] [Batch 50/57] D_loss: 0.0009, G_loss: 9.2090\nNo improvement in generator loss. Epochs without improvement: 7/15\n[Epoch 9/100] [Batch 0/57] D_loss: 0.0004, G_loss: 11.2022\n[Epoch 9/100] [Batch 10/57] D_loss: 0.0528, G_loss: 6.3750\n[Epoch 9/100] [Batch 20/57] D_loss: 0.0516, G_loss: 5.6084\n[Epoch 9/100] [Batch 30/57] D_loss: 0.4555, G_loss: 10.8501\n[Epoch 9/100] [Batch 40/57] D_loss: 0.0418, G_loss: 3.4635\n[Epoch 9/100] [Batch 50/57] D_loss: 0.0079, G_loss: 7.0926\nNo improvement in generator loss. Epochs without improvement: 8/15\n[Epoch 10/100] [Batch 0/57] D_loss: 0.0004, G_loss: 8.4934\n[Epoch 10/100] [Batch 10/57] D_loss: 0.0006, G_loss: 10.1206\n[Epoch 10/100] [Batch 20/57] D_loss: 0.0212, G_loss: 3.1787\n[Epoch 10/100] [Batch 30/57] D_loss: 0.0010, G_loss: 14.5447\n[Epoch 10/100] [Batch 40/57] D_loss: 0.0206, G_loss: 7.7807\n[Epoch 10/100] [Batch 50/57] D_loss: 2.2141, G_loss: 2.8322\nNo improvement in generator loss. Epochs without improvement: 9/15\n[Epoch 11/100] [Batch 0/57] D_loss: 0.4910, G_loss: 9.2475\n[Epoch 11/100] [Batch 10/57] D_loss: 0.4187, G_loss: 19.3730\n[Epoch 11/100] [Batch 20/57] D_loss: 0.0002, G_loss: 12.5924\n[Epoch 11/100] [Batch 30/57] D_loss: 1.1299, G_loss: 5.1624\n[Epoch 11/100] [Batch 40/57] D_loss: 0.0010, G_loss: 14.5047\n[Epoch 11/100] [Batch 50/57] D_loss: 0.0007, G_loss: 13.9889\nNo improvement in generator loss. Epochs without improvement: 10/15\n[Epoch 12/100] [Batch 0/57] D_loss: 0.2649, G_loss: 0.5056\n[Epoch 12/100] [Batch 10/57] D_loss: 0.0490, G_loss: 8.1636\n[Epoch 12/100] [Batch 20/57] D_loss: 0.1330, G_loss: 4.7877\n[Epoch 12/100] [Batch 30/57] D_loss: 0.0003, G_loss: 10.5184\n[Epoch 12/100] [Batch 40/57] D_loss: 0.8027, G_loss: 4.6349\n[Epoch 12/100] [Batch 50/57] D_loss: 1.5356, G_loss: 2.9391\nNo improvement in generator loss. Epochs without improvement: 11/15\n[Epoch 13/100] [Batch 0/57] D_loss: 0.9544, G_loss: 17.7291\n[Epoch 13/100] [Batch 10/57] D_loss: 0.0055, G_loss: 10.8396\n[Epoch 13/100] [Batch 20/57] D_loss: 0.0161, G_loss: 2.1261\n[Epoch 13/100] [Batch 30/57] D_loss: 0.1378, G_loss: 5.6501\n[Epoch 13/100] [Batch 40/57] D_loss: 0.5855, G_loss: 4.3786\n[Epoch 13/100] [Batch 50/57] D_loss: 0.0004, G_loss: 8.1619\nNo improvement in generator loss. Epochs without improvement: 12/15\n[Epoch 14/100] [Batch 0/57] D_loss: 0.0000, G_loss: 12.8713\n[Epoch 14/100] [Batch 10/57] D_loss: 0.0176, G_loss: 20.0301\n[Epoch 14/100] [Batch 20/57] D_loss: 0.0006, G_loss: 16.0094\n[Epoch 14/100] [Batch 30/57] D_loss: 0.0001, G_loss: 12.7049\n[Epoch 14/100] [Batch 40/57] D_loss: 0.1976, G_loss: 15.2209\n[Epoch 14/100] [Batch 50/57] D_loss: 0.0003, G_loss: 13.5588\nNo improvement in generator loss. Epochs without improvement: 13/15\n[Epoch 15/100] [Batch 0/57] D_loss: 0.0079, G_loss: 4.3499\n[Epoch 15/100] [Batch 10/57] D_loss: 0.0000, G_loss: 11.0396\n[Epoch 15/100] [Batch 20/57] D_loss: 1.1963, G_loss: 11.4389\n[Epoch 15/100] [Batch 30/57] D_loss: 0.1955, G_loss: 4.1684\n[Epoch 15/100] [Batch 40/57] D_loss: 0.0024, G_loss: 6.8275\n[Epoch 15/100] [Batch 50/57] D_loss: 0.0026, G_loss: 5.5389\nNo improvement in generator loss. Epochs without improvement: 14/15\n[Epoch 16/100] [Batch 0/57] D_loss: 0.0231, G_loss: 3.1207\n[Epoch 16/100] [Batch 10/57] D_loss: 0.1013, G_loss: 4.6597\n[Epoch 16/100] [Batch 20/57] D_loss: 2.2561, G_loss: 2.6007\n[Epoch 16/100] [Batch 30/57] D_loss: 0.6046, G_loss: 3.3702\n[Epoch 16/100] [Batch 40/57] D_loss: 0.0555, G_loss: 6.8501\n[Epoch 16/100] [Batch 50/57] D_loss: 0.0232, G_loss: 4.3949\nNo improvement in generator loss. Epochs without improvement: 15/15\nEarly stopping triggered at epoch 17\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}