{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11347934,"sourceType":"datasetVersion","datasetId":7100410}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\nprint(\"hello\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:07:30.867483Z","iopub.execute_input":"2025-04-16T19:07:30.867967Z","iopub.status.idle":"2025-04-16T19:07:32.129671Z","shell.execute_reply.started":"2025-04-16T19:07:30.867942Z","shell.execute_reply":"2025-04-16T19:07:32.129020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, utils\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Set seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Encoder class\nclass Encoder(nn.Module):\n    def __init__(self, in_channels=3, latent_dim=256):\n        super(Encoder, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, 64, 4, 2, 1)\n        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1)\n        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1)\n        self.conv4 = nn.Conv2d(256, 512, 4, 2, 1)\n        self.flatten = nn.Flatten()\n        self.fc_mu = nn.Linear(512 * 16 * 32, latent_dim)\n        self.fc_logvar = nn.Linear(512 * 16 * 32, latent_dim)\n\n    def forward(self, x):\n        skips = []\n        x = F.relu(self.conv1(x)); skips.append(x)\n        x = F.relu(self.conv2(x)); skips.append(x)\n        x = F.relu(self.conv3(x)); skips.append(x)\n        x = F.relu(self.conv4(x)); skips.append(x)\n        x = self.flatten(x)\n        mu = self.fc_mu(x)\n        logvar = self.fc_logvar(x)\n        return mu, logvar, skips\n\n# Decoder class\nclass Decoder(nn.Module):\n    def __init__(self, latent_dim):\n        super(Decoder, self).__init__()\n        self.fc = nn.Linear(latent_dim, 512 * 16 * 32)\n        self.deconv1 = nn.ConvTranspose2d(512, 256, 4, 2, 1)\n        self.deconv2 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n        self.deconv3 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n        self.output_layer = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n\n    def forward(self, z):\n        x = self.fc(z)\n        x = x.view(-1, 512, 16, 32)\n        x = F.relu(self.deconv1(x))\n        x = F.relu(self.deconv2(x))\n        x = F.relu(self.deconv3(x))\n        x = torch.sigmoid(self.output_layer(x))\n        return x\n\n# VAE\nclass VAE(nn.Module):\n    def __init__(self, latent_dim=256):\n        super(VAE, self).__init__()\n        self.encoder = Encoder(latent_dim=latent_dim)\n        self.decoder = Decoder(latent_dim=latent_dim)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        mu, logvar, skips = self.encoder(x)\n        z = self.reparameterize(mu, logvar)\n        recon = self.decoder(z)\n        return recon, mu, logvar\n\n    def generate(self, batch_size=1):\n        z = torch.randn(batch_size, 256).to(next(self.parameters()).device)\n        recon = self.decoder(z)\n        return recon\n\n# Loss Function (BCE + KL)\ndef vae_loss(recon_x, x, mu, logvar):\n    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + kl_div\n\n# Dataset Path and Transform\ndataset_path = '/kaggle/input/mangoleaf-dataset/dataset'\ntransform = transforms.Compose([\n    transforms.Resize((256, 512)),\n    transforms.ToTensor()\n])\n\n# Load base dataset for class filtering\nfull_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n\n# Helper: Train VAE per class\ndef train_vae_per_class(class_name, latent_dim=256, num_epochs=200, batch_size=4, patience=20):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    class_index = full_dataset.class_to_idx[class_name]\n    class_indices = [i for i, (_, label) in enumerate(full_dataset.samples) if label == class_index]\n    dataset = Subset(full_dataset, class_indices)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    vae = VAE(latent_dim=latent_dim).to(device)\n    optimizer = optim.Adam(vae.parameters(), lr=1e-4)\n\n    best_loss = float(\"inf\")\n    early_stopping_counter = 0\n    os.makedirs(\"vae_weights\", exist_ok=True)\n\n    print(f\"\\nTraining VAE for class: {class_name}\")\n    for epoch in range(num_epochs):\n        vae.train()\n        total_loss = 0\n        for images, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n            images = images.to(device)\n            optimizer.zero_grad()\n            recon, mu, logvar = vae(images)\n            loss = vae_loss(recon, images, mu, logvar)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(dataloader.dataset)\n        print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}\")\n\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            early_stopping_counter = 0\n            torch.save(vae.state_dict(), f\"vae_weights/vae_{class_name}.pth\")\n            print(f\"Saved best model for {class_name}\")\n        else:\n            early_stopping_counter += 1\n            if early_stopping_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n        # Visualize reconstruction\n        vae.eval()\n        with torch.no_grad():\n            sample_img = images[0].unsqueeze(0)\n            recon_img, _, _ = vae(sample_img)\n            comparison = torch.cat([sample_img.cpu(), recon_img.cpu()])\n            grid = utils.make_grid(comparison, nrow=2)\n            plt.figure(figsize=(10, 5))\n            plt.axis('off')\n            plt.title(f\"{class_name} - Epoch {epoch+1}\")\n            plt.imshow(grid.permute(1, 2, 0).clamp(0, 1))\n            plt.show()\n\n    return vae\n\n# Generate synthetic images\ndef generate_images(vae, batch_size=5):\n    vae.eval()\n    with torch.no_grad():\n        generated_images = vae.generate(batch_size=batch_size)\n        grid = utils.make_grid(generated_images, nrow=batch_size)\n        plt.figure(figsize=(batch_size * 2, 5))\n        plt.imshow(grid.permute(1, 2, 0).clamp(0, 1))\n        plt.axis('off')\n        plt.title(\"Generated Samples\")\n        plt.show()\n\n# Train for a class and generate\nvae = train_vae_per_class(class_name='DIEBACK', latent_dim=256, num_epochs=300, batch_size=4)\ngenerate_images(vae, batch_size=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:07:41.641761Z","iopub.execute_input":"2025-04-16T19:07:41.642425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import torch.optim as optim\n# from torch.utils.data import DataLoader\n# from torchvision import datasets, transforms, utils\n# from tqdm import tqdm\n# import matplotlib.pyplot as plt\n\n# # Encoder class\n# class Encoder(nn.Module):\n#     def __init__(self, in_channels=3, latent_dim=256):\n#         super(Encoder, self).__init__()\n#         self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1)  # 128x256\n#         self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)          # 64x128\n#         self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)         # 32x64\n#         self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)         # 16x32\n#         self.flatten = nn.Flatten()\n#         self.fc_mu = nn.Linear(512 * 16 * 32, latent_dim)\n#         self.fc_logvar = nn.Linear(512 * 16 * 32, latent_dim)\n\n#     def forward(self, x):\n#         skips = []\n#         x = F.relu(self.conv1(x)); skips.append(x)\n#         x = F.relu(self.conv2(x)); skips.append(x)\n#         x = F.relu(self.conv3(x)); skips.append(x)\n#         x = F.relu(self.conv4(x)); skips.append(x)\n#         x = self.flatten(x)\n#         mu = self.fc_mu(x)\n#         logvar = self.fc_logvar(x)\n#         return mu, logvar, skips\n\n# # Decoder class (Modified to ignore skip connections for generation)\n# class Decoder(nn.Module):\n#     def __init__(self, latent_dim):\n#         super(Decoder, self).__init__()\n#         self.fc = nn.Linear(latent_dim, 512 * 16 * 32)\n\n#         self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)  # 32x64\n#         self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)  # 64x128\n#         self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)    # 128x256\n#         self.output_layer = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)      # 256x512\n\n#     def forward(self, z, skips=None):\n#         x = self.fc(z)\n#         x = x.view(-1, 512, 16, 32)\n\n#         # Skip connections are ignored during generation\n#         x = F.relu(self.deconv1(x))\n#         x = F.relu(self.deconv2(x))\n#         x = F.relu(self.deconv3(x))\n\n#         # Generate final output\n#         x = torch.sigmoid(self.output_layer(x))\n#         return x\n\n# # VAE class\n# class VAE(nn.Module):\n#     def __init__(self, latent_dim=256):\n#         super(VAE, self).__init__()\n#         self.encoder = Encoder(latent_dim=latent_dim)\n#         self.decoder = Decoder(latent_dim=latent_dim)\n\n#     def reparameterize(self, mu, logvar):\n#         std = torch.exp(0.5 * logvar)\n#         eps = torch.randn_like(std)\n#         return mu + eps * std\n\n#     def forward(self, x):\n#         mu, logvar, skips = self.encoder(x)  # For training, we still use the encoder\n#         z = self.reparameterize(mu, logvar)\n#         recon = self.decoder(z)  # During training, decoder uses the latent vector from encoder\n#         return recon, mu, logvar\n\n#     def generate(self, batch_size=1):\n#         # For generation, we sample random noise (z) from a normal distribution\n#         z = torch.randn(batch_size, 256).to(next(self.parameters()).device)\n#         recon = self.decoder(z)  # Generate images from random noise\n#         return recon\n\n# # VAE Loss Function\n# def vae_loss(recon_x, x, mu, logvar):\n#     recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n#     kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n#     return recon_loss + kl_div\n\n# # Dataset Loading\n# image_size = (512, 1024)\n# transform = transforms.Compose([\n#     transforms.Resize(image_size),\n#     transforms.ToTensor(),\n# ])\n\n# dataset_path = '/kaggle/input/mangoleaf-dataset/dataset'\n# dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n# class_names = dataset.classes\n# print(\"Loaded classes:\", class_names)\n\n# # Training function\n# def train_vae_per_class(class_name, dataset_path, image_size=(256, 512), latent_dim=256, num_epochs=200, batch_size=2, patience=20):\n#     transform = transforms.Compose([\n#         transforms.Resize(image_size),\n#         transforms.ToTensor()\n#     ])\n\n#     class_dir = os.path.join(dataset_path, class_name)\n#     dataset = datasets.ImageFolder(root=os.path.join(dataset_path), transform=transform)\n#     filtered_dataset = [d for d in dataset.samples if class_name in d[0]]\n#     dataset.samples = filtered_dataset\n#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     vae = VAE(latent_dim=latent_dim).to(device)\n#     optimizer = torch.optim.Adam(vae.parameters(), lr=1e-4)\n\n#     best_loss = float(\"inf\")\n#     early_stopping_counter = 0\n#     os.makedirs(\"vae_weights\", exist_ok=True)\n\n#     print(f\"\\nTraining VAE for class: {class_name}\")\n#     for epoch in range(num_epochs):\n#         vae.train()\n#         total_loss = 0\n#         for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n#             images, _ = batch\n#             images = images.to(device)\n#             optimizer.zero_grad()\n#             recon_images, mu, logvar = vae(images)\n#             loss = vae_loss(recon_images, images, mu, logvar)\n#             loss.backward()\n#             optimizer.step()\n#             total_loss += loss.item()\n\n#         avg_loss = total_loss / len(dataloader)\n#         print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.2f}\")\n\n#         # Early Stopping\n#         if avg_loss < best_loss:\n#             best_loss = avg_loss\n#             early_stopping_counter = 0\n#             torch.save(vae.state_dict(), f\"vae_weights/vae_{class_name}.pth\")\n#             print(f\"Saved best model for {class_name} with loss {best_loss:.2f}\")\n#         else:\n#             early_stopping_counter += 1\n#             print(f\"No improvement. Early stopping counter: {early_stopping_counter}/{patience}\")\n#             if early_stopping_counter >= patience:\n#                 print(f\"Early stopping at epoch {epoch+1}\")\n#                 break\n\n#         # Generate and show image\n#         vae.eval()\n#         with torch.no_grad():\n#             sample_img = images[0].unsqueeze(0)\n#             recon_img, _, _ = vae(sample_img)\n#             comparison = torch.cat([sample_img.cpu(), recon_img.cpu()])\n#             grid = utils.make_grid(comparison, nrow=2)\n#             plt.figure(figsize=(10, 5))\n#             plt.axis('off')\n#             plt.title(f\"{class_name} - Epoch {epoch+1}\")\n#             plt.imshow(grid.permute(1, 2, 0))\n#             plt.show()\n\n#     return vae\n\n# # Usage: Generate random images after training\n# def generate_images(vae, batch_size=5):\n#     vae.eval()\n#     with torch.no_grad():\n#         generated_images = vae.generate(batch_size=batch_size)\n#         grid = utils.make_grid(generated_images, nrow=batch_size)\n#         plt.figure(figsize=(10, 5))\n#         plt.imshow(grid.permute(1, 2, 0))\n#         plt.axis('off')\n#         plt.show()\n\n# # Example: Train the VAE for a specific class\n# vae = train_vae_per_class(class_name='DIEBACK', dataset_path=dataset_path, image_size=(256, 512), latent_dim=256, num_epochs=300, batch_size=2)\n\n# # After training, you can generate synthetic images\n# generate_images(vae, batch_size=5)  # Generate and show 5 images from random noise\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_images(vae, batch_size=5):\n    vae.eval()\n    with torch.no_grad():\n        generated_images = vae.generate(batch_size=batch_size)\n        grid = utils.make_grid(generated_images, nrow=batch_size)\n        plt.figure(figsize=(10, 5))\n        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())  # move to CPU before displaying\n        plt.axis('off')\n        plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_images(vae, batch_size=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}